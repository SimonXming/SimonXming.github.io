---
layout: post
title: 人工神经网络 (ANN) 简述
category: 系列
tags: 深度学习
keywords: 深度学习 ANN
description:
---

我们从下面四点认识人工神经网络（ANN: Artificial Neutral Network）：
* 神经元结构
* 神经元的激活函数
* 神经网络拓扑结构
* 神经网络选择权值和学习算法

## 1. 神经元：

我们先来看一组对比图就能了解是怎样从生物神经元建模为人工神经元。

![人工神经元建模过程](http://www.worktle.com/wp-content/uploads/2017/11/a-r1.png)

下面分别讲述:

生物神经元的组成包括细胞体、树突、轴突、突触。  
* 树突可以看作输入端，接收从其他细胞传递过来的电信号；  
* 轴突可以看作输出端，传递电荷给其他细胞；  
* 突触可以看作 I/O 接口，连接神经元，单个神经元可以和上千个神经元连接。  
* 细胞体内有膜电位，从外界传递过来的电流使膜电位发生变化，并且不断累加，当膜电位升高到超过一个阈值时，神经元被激活，产生一个脉冲，传递到下一个神经元。


为了更形象理解神经元传递信号过程，把一个神经元比作一个水桶。  
水桶下侧连着多根水管（树突），水管既可以把桶里的水排出去（抑制性），又可以将其他水桶的水输进来（兴奋性），水管的粗细不同，对桶中水的影响程度不同（权重），水管对水桶水位（膜电位）的改变就是水桶内水位的改变，当桶中水达到一定高度时，就能通过另一条管道（轴突）排出去。

![神经元](http://www.worktle.com/wp-content/uploads/2017/11/a-r2.png)

神经元是多输入单输出的信息处理单元，具有空间整合性和阈值性，输入分为兴奋性输入和抑制性输入。

按照这个原理，科学家提出了 M-P 模型（取自两个提出者的姓名首字母），M-P 模型是对生物神经元的建模，作为人工神经网络中的一个神经元。

![MP 模型的示意图](http://www.worktle.com/wp-content/uploads/2017/11/a-r3.png)

由 MP 模型的示意图，我们可以看到与生物神经元的相似之处，x_i 表示多个输入，W_ij 表示每个输入的权值，其正负模拟了生物神经元中突出的兴奋和抑制；sigma 表示将全部输入信号进行累加整合，f 为激活函数，O 为输出。下图可以看到生物神经元和 MP 模型的类比：

![生物神经元和 MP 模型的类比](http://www.worktle.com/wp-content/uploads/2017/11/a-r5.png)

往后诞生的各种神经元模型都是由 MP 模型演变过来。

## 2. 激活函数

激活函数可以看作滤波器，接收外界各种各样的信号，通过调整函数，输出期望值。ANN 通常采用三类激活函数: 阈值函数、分段函数、双极性连续函数（sigmoid，tanh）：

![三类激活函数](http://www.worktle.com/wp-content/uploads/2017/11/a-r6.png)

## 3. 学习算法

神经网络的学习也称为训练，通过神经网络所在环境的刺激作用调整神经网络的自由参数（如连接权值），使神经网络以一种新的方式对外部环境做出反应的一个过程。每个神经网络都有一个激活函数 y=f(x)，训练过程就是通过给定的海量 x 数据和 y 数据，拟合出激活函数 f。学习过程分为有导师学习和无导师学习，有导师学习是给定期望输出，通过对权值的调整使实际输出逼近期望输出；无导师学习给定表示方法质量的测量尺度，根据该尺度来优化参数。常见的有 Hebb 学习、纠错学习、基于记忆学习、随机学习、竞争学习。

* Hebb 学习：  
（贴公式不方便，只简述原理）这是最早提出的学习方法，原理是如果突触（连接）两边的两个神经元被同时（同步）激活，则该突触的能量（权重）就选择性增加；如果被异步激活，则该突出能量减弱或消除。
* 纠错学习：  
计算实际输出和期望输出的误差，再返回误差，修改权值。原理简单，用到最多，最小梯度下降法（LMS 最小均方误差算法）就是这种方法。
* 基于记忆的学习：  
主要用于模式分类，在基于记忆的学习中，过去的学习结果被存储在一个大的存储器中，当输入一个新的测试向量时，学习过程就是把新向量归到已存储的某个类中。算法包括两部分：一是用于定义测试向量局部领域的标准；二是在局部领域训练样本的学习规则。常用最近邻规则。
* 随机学习算法：  
也叫 Bolzmann 学习规则，根据最大似然规则，通过调整权值，最小化似然函数或其对数。
模拟退火算法是从物理和化学退火过程类推过来，是 “对物体加温后再冷却的处理过程” 的数学建模。整个过程分为两步：首先在高温下进行搜索，此时各状态出现概率相差不大，可以很快进入 “热平衡状态”，这时进行的是 “粗搜索”，也就是大致找到系统的低能区区域；随着温度降低，各状态出现的概率差距逐渐被扩大，搜索精度不断提高，这就可以越来越准确地找到网络能量函数的全局最小点。
* 竞争学习：  
神经网络的输出神经元之间相互竞争，在任一时间只能有一个输出神经元是活性的。

## 4. 神经网络拓扑结构

常见的拓扑结构有单层前向网络、多层前向网络、反馈网络，随机神经网络、竞争神经网络。

![单层前向网络、多层前向网络、反馈网络](http://www.worktle.com/wp-content/uploads/2017/11/a-r7.png)
![随机神经网络、竞争神经网络](http://www.worktle.com/wp-content/uploads/2017/11/a-r8.png)

## 5. 神经网络的发展
略

[原文 worktle.com - 人工神经网络](http://www.worktle.com/articles/6601/)
